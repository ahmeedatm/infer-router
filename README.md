# Infer Router

Ce d√©p√¥t contient une API FastAPI nomm√©e "Infer Router" qui route des demandes d'inf√©rence vers des mod√®les simul√©s en fonction de la charge de la file d'attente (Redis). Le but principal est de d√©montrer une logique de routage adaptative (privil√©gier latence vs pr√©cision) selon la taille de la file.

**Structure du projet**

- app/
  - main.py : impl√©mentation principale de l'API FastAPI, gestion du cycle de vie et du worker d'inf√©rence.
- docker-compose.yml : (optionnel) composition pour lancer l'API et Redis.
- Dockerfile : image de l'application.
- requirements.txt : d√©pendances Python.

**R√©sum√© des fonctionnalit√©s impl√©ment√©es**

- **Serveur HTTP**: API FastAPI exposant des endpoints REST.
- **Queue Redis (asyncio)**: les requ√™tes d'inf√©rence sont pouss√©es dans la liste Redis `inference_queue`.
- **Worker asynchrone**: une t√¢che de fond lit la file avec `BRPOP` et ex√©cute une inf√©rence simul√©e.
- **Routage adaptatif**: si la longueur de la file d√©passe un seuil (`QUEUE_THRESHOLD = 5`), le worker choisit un mod√®le rapide (`Fast-Model`) pour privil√©gier la latence ; sinon il choisit un mod√®le pr√©cis (`Accurate-Model`).
- **Historique des r√©sultats**: chaque inf√©rence aboutie est pouss√©e dans `inference_results` (liste Redis) avec m√©tadonn√©es (id du capteur, mod√®le utilis√©, latence, longueur de file au d√©part).
- **Lifespan FastAPI**: cr√©ation et fermeture du client Redis et lancement/arr√™t propre du worker via un `asynccontextmanager`.

**Fichiers cl√©s et explication**

- `app/main.py` :
  - Import des biblioth√®ques (`FastAPI`, `uvicorn`, `pydantic`, `redis.asyncio`, `asyncio`, etc.).
  - D√©finition du mod√®le Pydantic `InferenceRequest` avec `sensor_id`, `timestamp` et `features`.
  - Fonction `process_inference(redis_client)` : boucle infinie qui lit `inference_queue`, calcule la longueur de la file, choisit `Fast-Model` ou `Accurate-Model`, simule un temps de calcul (`0.5s` ou `2.0s`), calcule la latence et stocke l'historique dans `inference_results`.
  - `lifespan(app)` : initialise `app.state.redis = Redis(host="redis", port=6379)`, lance `process_inference` en t√¢che de fond et effectue le nettoyage (annulation de la t√¢che et fermeture du client Redis).
  - Endpoints :
    - `GET /` : message de bienvenue.
    - `GET /health` : √©tat `ok`.
    - `GET /results` : retourne les 10 derniers r√©sultats depuis `inference_results`.
    - `POST /data` : re√ßoit `InferenceRequest`, s√©rialise en JSON et fait `LPUSH inference_queue`.

**Comment l'ex√©cuter localement**

M√©thode 1 ‚Äî avec `uvicorn` (environnement virtuel activ√©) :

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

M√©thode 2 ‚Äî avec Docker / docker-compose (si `docker-compose.yml` fourni) :

```bash
docker-compose up --build
```

(Remarque : le service Redis doit √™tre joignable √† l'h√¥te `redis` sur le port `6379` si vous utilisez la configuration par d√©faut du projet.)

**Exemples d'usage (tests rapides)**

1) Poster une donn√©e d'inf√©rence :

```bash
curl -X POST "http://localhost:8000/data" -H "Content-Type: application/json" -d '
{
  "sensor_id": "sensor-01",
  "timestamp": 1670000000.0,
  "features": [0.1, 0.2, 0.3]
}'
```

2) R√©cup√©rer les derniers r√©sultats :

```bash
curl http://localhost:8000/results
```

3) Simulation rapide pour d√©clencher le mode d√©grad√© : envoyer plusieurs requ√™tes POST en parall√®le (ou utiliser un petit script loop) pour augmenter la longueur de la file au-dessus du seuil `5`.

**D√©pendances importantes**

- fastapi
- uvicorn
- redis (version 4.x+ avec support `redis.asyncio`)
- pydantic

Ces d√©pendances doivent √™tre list√©es dans `requirements.txt`.

**Remarques d'impl√©mentation & limites**

- Les mod√®les (`Fast-Model`, `Accurate-Model`) sont simul√©s par des `asyncio.sleep`. Il faut remplacer cette simulation par des appels r√©els d'inf√©rence (HTTP RPC, gRPC, chargement local de mod√®le) pour un usage en production.
- L'utilisation actuelle de `BRPOP` bloque jusqu'√† r√©ception : c'est volontaire pour la logique du worker. En production, on peut envisager un timeout, gestion d'exceptions r√©seau et backoff.
- La m√©trique `latency` est calcul√©e comme `time.time() - timestamp` envoy√© par le client. Veillez √† synchroniser les horloges (NT P) si la latence est critique.
- Gestion des erreurs et des cas limites (messages malform√©s, Redis down) doit √™tre renforc√©e (retries, dead-letter queue, logs structur√©s).

**Am√©liorations possibles**

- Ajouter des tests unitaires et d'int√©gration.
- Exposer des m√©triques Prometheus (latences, longueur de file, mod√®le s√©lectionn√©).
- Remplacer la simulation des mod√®les par une int√©gration r√©elle (TensorFlow/PyTorch, ou requ√™tes vers des microservices d'inf√©rence).
- Param√©trer `QUEUE_THRESHOLD` via variable d'environnement ou configuration.
- Ajouter authentification/autorisation pour l'API si n√©cessaire.

**To-Do List consolid√©e (version acad√©mique)**

Voici la To-Do List consolid√©e de ton projet InferRouter. Elle est structur√©e pour coller parfaitement √† ton sujet acad√©mique.

‚úÖ **Phase 1 : Infrastructure & M√©canique de base (Termin√©)**
Tout ce qui concerne la "plomberie" du syst√®me est op√©rationnel.

- [x] Mise en place Docker : Conteneurs API et Redis qui communiquent dans un r√©seau isol√© (`docker-compose`).
- [x] API d'Ingestion : Route `POST /data` qui re√ßoit les donn√©es JSON et valide le format (Pydantic).
- [x] Syst√®me de File d'Attente : S√©rialisation des requ√™tes et stockage dans une liste Redis (`LPUSH`).
- [x] Worker Asynchrone : T√¢che de fond qui d√©pile les messages (`BRPOP`) sans bloquer l'API.
- [x] Simulation des Mod√®les : Utilisation de `asyncio.sleep()` pour simuler des temps de calcul diff√©rents (Fast vs Accurate).

‚úÖ **Phase 2 : M√©triques & Premi√®re Intelligence (Termin√©)**
Le syst√®me est capable de s'observer et de r√©agir √† la charge.

- [x] Calcul de Latence : Mesure du temps total ($T_{fin} - T_{d√©but}$) pour chaque requ√™te.
- [x] Historisation : Sauvegarde des r√©sultats (latence, mod√®le utilis√©) dans Redis (`inference_results`).
- [x] Visualisation : Route `GET /results` pour consulter l'historique depuis Postman.
- [x] Contr√¥le par Seuil (Charge) : Le Worker v√©rifie la taille de la file (`LLEN`). Si file > 5, il bascule automatiquement sur le mod√®le rapide.

üìù **Phase 3 : Profilage Dynamique & Feedback (√Ä FAIRE)**
C'est l'√©tape actuelle. Le syst√®me doit apprendre de ses erreurs (R√©troaction).

- [ ] Gestion de l'√©tat des mod√®les : Stocker la pr√©cision actuelle de chaque mod√®le dans Redis (ex: `accuracy:Fast-Model = 0.60`).
- [ ] Route de Feedback : Cr√©er une route `POST /feedback` permettant √† un utilisateur (ou simulateur) de dire "La pr√©cision de ce mod√®le a baiss√©".
- [ ] Int√©gration dans le Worker : Le Worker doit r√©cup√©rer la pr√©cision actuelle du mod√®le choisi au moment du calcul pour l'enregistrer dans l'historique.

üìù **Phase 4 : Strat√©gie de Priorisation (√Ä FAIRE)**
Le "Cerveau" final. Il doit prendre une d√©cision bas√©e sur DEUX crit√®res : la file d'attente ET la pr√©cision.

- [ ] Algorithme de d√©cision : Impl√©menter une logique un peu plus fine.

  Exemple : "Si la file est vide MAIS que le mod√®le pr√©cis est devenu mauvais (feedback < 0.5), alors utiliser le mod√®le rapide quand m√™me."
- [ ] Mode D√©grad√© : S'assurer que le syst√®me ne plante pas si les pr√©cisions ne sont pas d√©finies.

üìù **Phase 5 : Comparaison & Rapport (√Ä FAIRE)**
La preuve scientifique pour ton rendu.

- [ ] Sc√©nario "T√©moin 1" : Faire un test en for√ßant le syst√®me √† utiliser toujours le mod√®le `Accurate` (et voir la latence exploser).
- [ ] Sc√©nario "T√©moin 2" : Faire un test avec toujours le mod√®le `Fast` (latence basse, mais pr√©cision faible).
- [ ] Sc√©nario "InferRouter" : Faire le test avec ton algorithme (latence ma√Ætris√©e + pr√©cision optimis√©e).
- [ ] Export des donn√©es : R√©cup√©rer le JSON de `/results` pour en faire un graphique (Excel ou Python/Matplotlib) comparant les 3 courbes.